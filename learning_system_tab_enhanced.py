"""
Module am√©lior√© pour l'onglet Syst√®me d'Apprentissage d'ArcanShadow.
Ce module visualise l'√©volution de l'intelligence du syst√®me et ses processus d'apprentissage,
avec une int√©gration des donn√©es multi-sources pour am√©liorer les analyses.
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
from datetime import datetime, timedelta
import random
import logging

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Importer le hub d'int√©gration de donn√©es
from api.data_integration_hub import DataIntegrationHub

# Importer les composants am√©lior√©s
try:
    from modules.enhanced_components import get_enhanced_components
    enhanced_components = get_enhanced_components()
    
    # R√©cup√©ration des composants am√©lior√©s
    FanSentimentMonitorEnhanced = enhanced_components.get_component('fan_sentiment_monitor')
    ShadowOddsPlusEnhanced = enhanced_components.get_component('shadow_odds_plus')
except ImportError:
    FanSentimentMonitorEnhanced = None
    ShadowOddsPlusEnhanced = None

def generate_learning_data(days=30):
    """
    G√©n√®re des donn√©es avanc√©es d'apprentissage du syst√®me int√©grant les multiples sources de donn√©es.
    
    Args:
        days (int): Nombre de jours d'historique √† g√©n√©rer
        
    Returns:
        dict: Donn√©es d'apprentissage du syst√®me
    """
    # Initialisation du hub d'int√©gration
    data_hub = DataIntegrationHub()
    
    # Initialiser les composants am√©lior√©s si disponibles
    sentiment_analyzer = FanSentimentMonitorEnhanced if FanSentimentMonitorEnhanced else None
    
    # Date de d√©part (30 jours avant aujourd'hui par d√©faut)
    start_date = datetime.now() - timedelta(days=days)
    
    # Dates pour chaque jour
    dates = [start_date + timedelta(days=i) for i in range(days+1)]
    
    # Donn√©es de pr√©cision des pr√©dictions
    base_accuracy = 0.65  # Pr√©cision initiale
    final_accuracy = 0.88  # Pr√©cision finale attendue
    
    # G√©n√©ration de la courbe d'apprentissage avec diff√©rentes phases
    accuracies = []
    
    # Phase 1: Croissance initiale rapide
    phase1_days = days // 5
    phase1_increment = (0.75 - base_accuracy) / phase1_days
    for i in range(phase1_days):
        accuracies.append(base_accuracy + phase1_increment * i + random.uniform(-0.02, 0.02))
    
    # Phase 2: Plateau/stabilisation
    phase2_days = days // 5
    phase2_base = accuracies[-1]
    for i in range(phase2_days):
        accuracies.append(phase2_base + random.uniform(-0.01, 0.01))
    
    # Phase 3: Croissance lente
    phase3_days = days // 5
    phase3_base = accuracies[-1]
    phase3_increment = (0.82 - phase3_base) / phase3_days
    for i in range(phase3_days):
        accuracies.append(phase3_base + phase3_increment * i + random.uniform(-0.015, 0.015))
    
    # Phase 4: Int√©gration de nouvelles sources (bond de performance)
    phase4_days = days // 5
    phase4_base = accuracies[-1]
    # Simuler un bond de performance lors de l'int√©gration des donn√©es multi-sources
    phase4_jump = 0.04  # Bond de 4% de performance
    for i in range(phase4_days):
        if i == 0:
            accuracies.append(phase4_base + phase4_jump + random.uniform(-0.01, 0.01))
        else:
            accuracies.append(accuracies[-1] + random.uniform(-0.01, 0.01))
    
    # Phase 5: Optimisation finale
    phase5_days = days - len(accuracies)
    phase5_base = accuracies[-1]
    phase5_increment = (final_accuracy - phase5_base) / max(1, phase5_days)
    for i in range(phase5_days):
        accuracies.append(phase5_base + phase5_increment * i + random.uniform(-0.01, 0.01))
    
    # G√©n√©ration du nombre de patterns identifi√©s
    initial_patterns = 520
    final_patterns = 1250
    
    # Progression similaire mais avec des sauts plus marqu√©s pour les patterns
    patterns = []
    total_increment = final_patterns - initial_patterns
    
    # Phase 1: D√©couverte initiale
    phase1_patterns = initial_patterns + int(total_increment * 0.2)
    phase1_increment = (phase1_patterns - initial_patterns) / phase1_days
    for i in range(phase1_days):
        patterns.append(int(initial_patterns + phase1_increment * i + random.uniform(-5, 5)))
    
    # Phase 2: Stabilisation/consolidation
    phase2_patterns_increment = int(total_increment * 0.05) / phase2_days
    for i in range(phase2_days):
        patterns.append(int(patterns[-1] + phase2_patterns_increment + random.uniform(-2, 2)))
    
    # Phase 3: D√©couverte progressive
    phase3_patterns_increment = int(total_increment * 0.15) / phase3_days
    for i in range(phase3_days):
        patterns.append(int(patterns[-1] + phase3_patterns_increment + random.uniform(-3, 3)))
    
    # Phase 4: Bond majeur avec l'int√©gration des nouvelles sources
    phase4_patterns_jump = int(total_increment * 0.4)
    phase4_patterns_base = patterns[-1]
    for i in range(phase4_days):
        if i == 0:
            patterns.append(int(phase4_patterns_base + phase4_patterns_jump))
        else:
            patterns.append(int(patterns[-1] + random.uniform(-1, 3)))
    
    # Phase 5: Optimisation et raffinement
    phase5_patterns_increment = (final_patterns - patterns[-1]) / max(1, phase5_days)
    for i in range(phase5_days):
        patterns.append(int(patterns[-1] + phase5_patterns_increment + random.uniform(-2, 2)))
    
    # G√©n√©ration des √©v√©nements d'apprentissage
    events = []
    event_types = [
        "Recalibration des param√®tres",
        "Mise √† jour des poids",
        "Identification d'un nouveau pattern",
        "Int√©gration de donn√©es externes",
        "Ajustement auto-adaptatif"
    ]
    
    # Ajout d'√©v√©nements sp√©cifiques aux modules am√©lior√©s
    if FanSentimentMonitorEnhanced:
        event_types.append("Analyse avanc√©e des sentiments des fans")
    
    if ShadowOddsPlusEnhanced:
        event_types.append("D√©tection d'anomalies dans les cotes")
        
    # G√©n√©rer des √©v√©nements tout au long de la p√©riode
    event_count = days // 2  # Un √©v√©nement tous les 2 jours en moyenne
    event_timestamps = sorted(random.sample(range(days), event_count))
    
    for day in event_timestamps:
        event_date = dates[day]
        event_type = random.choice(event_types)
        
        # Description sp√©cifique selon le type d'√©v√©nement
        event_desc = ""
        if event_type == "Recalibration des param√®tres":
            event_desc = f"Recalibration des param√®tres du module {random.choice(['Pr√©dictionsEnhanced', 'BetTrapMapEnhanced', 'AnalyseEnhanced'])}"
        elif event_type == "Mise √† jour des poids":
            event_desc = f"Mise √† jour des poids du r√©seau neuronal suite √† l'analyse de {random.randint(50, 200)} nouveaux matchs"
        elif event_type == "Identification d'un nouveau pattern":
            pattern_choices = ["tendance de sur-performance des √©quipes apr√®s un changement d'entra√Æneur", 
                             "corr√©lation entre m√©t√©o et performance des √©quipes techniques", 
                             "impact des suspensions sur les phases d√©fensives"]
            event_desc = f"Nouveau pattern identifi√©: {random.choice(pattern_choices)}"
        elif event_type == "Int√©gration de donn√©es externes":
            event_desc = f"Int√©gration de nouvelles donn√©es de {random.choice(['Transfermarkt', 'soccerdata', 'donn√©es d√©taill√©es des joueurs'])}"
        elif event_type == "Ajustement auto-adaptatif":
            event_desc = f"Ajustement auto-adaptatif du seuil de confiance dans les pr√©dictions {random.choice(['de victoire √† domicile', 'de matchs √† haut score', 'de clean sheets'])}"
        elif event_type == "Analyse avanc√©e des sentiments des fans":
            event_desc = f"Corr√©lation √©tablie entre sentiment des fans et performance de l'√©quipe pour {random.choice(['les √©quipes de Premier League', 'les clubs avec forte pr√©sence sur les r√©seaux sociaux', 'les derbies √† fort enjeu'])}"
        elif event_type == "D√©tection d'anomalies dans les cotes":
            event_desc = f"D√©tection d'un pattern r√©current d'anomalies dans les cotes de {random.choice(['matchs de fin de saison', '√©quipes en lute pour le maintien', 'comp√©titions mineures'])}"
        
        events.append({
            "date": event_date,
            "type": event_type,
            "description": event_desc,
            "impact": random.uniform(0.01, 0.05)  # Impact de l'√©v√©nement sur la performance
        })
    
    # Cr√©ation des donn√©es de performance des modules
    modules_data = [
        {
            "name": "ArcanPredictEnhanced",
            "accuracy": random.uniform(0.85, 0.92),
            "pattern_recognition": random.uniform(0.80, 0.90),
            "learning_speed": random.uniform(0.75, 0.85),
            "data_integration": random.uniform(0.85, 0.95),
            "anomaly_detection": random.uniform(0.75, 0.85)
        },
        {
            "name": "BetTrapMapEnhanced",
            "accuracy": random.uniform(0.80, 0.88),
            "pattern_recognition": random.uniform(0.85, 0.95),
            "learning_speed": random.uniform(0.70, 0.80),
            "data_integration": random.uniform(0.90, 0.98),
            "anomaly_detection": random.uniform(0.85, 0.95)
        },
        {
            "name": "ShadowOddsPlusEnhanced",
            "accuracy": random.uniform(0.83, 0.93),
            "pattern_recognition": random.uniform(0.75, 0.85),
            "learning_speed": random.uniform(0.80, 0.90),
            "data_integration": random.uniform(0.75, 0.85),
            "anomaly_detection": random.uniform(0.90, 0.98)
        },
        {
            "name": "FanSentimentMonitorEnhanced",
            "accuracy": random.uniform(0.75, 0.85),
            "pattern_recognition": random.uniform(0.85, 0.92),
            "learning_speed": random.uniform(0.85, 0.95),
            "data_integration": random.uniform(0.80, 0.90),
            "anomaly_detection": random.uniform(0.70, 0.85)
        }
    ]
    
    # Cr√©ation d'un DataFrame des donn√©es quotidiennes
    daily_data = pd.DataFrame({
        'date': dates,
        'accuracy': accuracies,
        'patterns': patterns
    })
    
    # Calcul des √©v√©nements par jour pour le graphique
    events_per_day = {}
    for event in events:
        day = event['date'].strftime('%Y-%m-%d')
        if day in events_per_day:
            events_per_day[day] += 1
        else:
            events_per_day[day] = 1
    
    daily_data['events_count'] = daily_data['date'].apply(
        lambda x: events_per_day.get(x.strftime('%Y-%m-%d'), 0)
    )
    
    return {
        'daily_data': daily_data,
        'events': events,
        'modules': modules_data
    }

def create_accuracy_chart(data):
    """
    Cr√©e un graphique avanc√© d'√©volution de la pr√©cision des pr√©dictions.
    
    Args:
        data (pd.DataFrame): Donn√©es quotidiennes d'apprentissage
        
    Returns:
        plotly.graph_objects.Figure: Graphique g√©n√©r√©
    """
    fig = go.Figure()
    
    # Ligne principale de pr√©cision
    fig.add_trace(go.Scatter(
        x=data['date'],
        y=data['accuracy'],
        mode='lines',
        name='Pr√©cision des pr√©dictions',
        line=dict(color='rgba(163, 119, 254, 0.8)', width=3),
        hovertemplate='%{x|%d %b %Y}: %{y:.1%}<extra></extra>'
    ))
    
    # Ajouter une ligne de tendance
    x_numeric = np.arange(len(data['date']))
    z = np.polyfit(x_numeric, data['accuracy'], 1)
    p = np.poly1d(z)
    
    fig.add_trace(go.Scatter(
        x=data['date'],
        y=p(x_numeric),
        mode='lines',
        name='Tendance',
        line=dict(color='rgba(255, 170, 50, 0.6)', width=2, dash='dash'),
        hovertemplate='%{x|%d %b %Y}: %{y:.1%}<extra></extra>'
    ))
    
    # Marquer les points d'int√©gration des nouvelles sources de donn√©es
    # D√©tection automatique des "sauts" de performance
    accuracy_diff = data['accuracy'].diff()
    significant_jumps = data[accuracy_diff > 0.03]
    
    if not significant_jumps.empty:
        fig.add_trace(go.Scatter(
            x=significant_jumps['date'],
            y=significant_jumps['accuracy'],
            mode='markers',
            name='Int√©gration de nouvelles sources',
            marker=dict(
                color='rgba(255, 100, 100, 0.8)',
                size=12,
                symbol='star'
            ),
            hovertemplate='%{x|%d %b %Y}: Bond de performance<extra></extra>'
        ))
    
    # Mise en forme du graphique
    fig.update_layout(
        title="√âvolution de la pr√©cision des pr√©dictions",
        xaxis_title="Date",
        yaxis_title="Pr√©cision",
        yaxis=dict(
            tickformat='.0%',
            range=[0.6, 0.95]
        ),
        hovermode="x unified",
        height=400,
        margin=dict(l=20, r=20, t=40, b=20)
    )
    
    return fig

def create_patterns_chart(data):
    """
    Cr√©e un graphique avanc√© d'√©volution du nombre de patterns identifi√©s.
    
    Args:
        data (pd.DataFrame): Donn√©es quotidiennes d'apprentissage
        
    Returns:
        plotly.graph_objects.Figure: Graphique g√©n√©r√©
    """
    fig = go.Figure()
    
    # Ligne principale des patterns
    fig.add_trace(go.Scatter(
        x=data['date'],
        y=data['patterns'],
        mode='lines',
        name='Patterns identifi√©s',
        line=dict(color='rgba(46, 134, 193, 0.8)', width=3),
        hovertemplate='%{x|%d %b %Y}: %{y} patterns<extra></extra>'
    ))
    
    # D√©tection des sauts significatifs dans le nombre de patterns
    patterns_diff = data['patterns'].diff()
    significant_jumps = data[patterns_diff > 50]  # Sauts de plus de 50 patterns
    
    if not significant_jumps.empty:
        fig.add_trace(go.Scatter(
            x=significant_jumps['date'],
            y=significant_jumps['patterns'],
            mode='markers',
            name='Bonds majeurs',
            marker=dict(
                color='rgba(26, 188, 156, 0.8)',
                size=12,
                symbol='circle'
            ),
            hovertemplate='%{x|%d %b %Y}: %{y} patterns<br>Bond significatif<extra></extra>'
        ))
    
    # Mise en forme du graphique
    fig.update_layout(
        title="√âvolution du nombre de patterns identifi√©s",
        xaxis_title="Date",
        yaxis_title="Nombre de patterns",
        hovermode="x unified",
        height=400,
        margin=dict(l=20, r=20, t=40, b=20)
    )
    
    return fig

def create_module_radar_chart(modules):
    """
    Cr√©e un graphique radar avanc√© comparant les performances des modules.
    
    Args:
        modules (list): Liste des modules et leurs m√©triques
        
    Returns:
        plotly.graph_objects.Figure: Graphique g√©n√©r√©
    """
    categories = ['Pr√©cision', 'Reconnaissance de patterns', 'Vitesse d\'apprentissage', 
                 'Int√©gration des donn√©es', 'D√©tection d\'anomalies']
    
    fig = go.Figure()
    
    colors = ['rgba(163, 119, 254, 0.7)', 'rgba(46, 134, 193, 0.7)', 
              'rgba(26, 188, 156, 0.7)', 'rgba(241, 196, 15, 0.7)']
    
    for i, module in enumerate(modules):
        fig.add_trace(go.Scatterpolar(
            r=[
                module['accuracy'], 
                module['pattern_recognition'], 
                module['learning_speed'], 
                module['data_integration'], 
                module['anomaly_detection']
            ],
            theta=categories,
            fill='toself',
            name=module['name'],
            line=dict(color=colors[i % len(colors)], width=2),
            opacity=0.8
        ))
    
    fig.update_layout(
        polar=dict(
            radialaxis=dict(
                visible=True,
                range=[0.6, 1]
            ),
            angularaxis_tickfont_size=12
        ),
        showlegend=True,
        title="Performance comparative des modules",
        height=450,
        margin=dict(l=80, r=80, t=40, b=40)
    )
    
    return fig

def create_learning_events_chart(data, events):
    """
    Cr√©e un graphique avanc√© des √©v√©nements d'apprentissage.
    
    Args:
        data (pd.DataFrame): Donn√©es quotidiennes d'apprentissage
        events (list): Liste des √©v√©nements d'apprentissage
        
    Returns:
        plotly.graph_objects.Figure: Graphique g√©n√©r√©
    """
    fig = go.Figure()
    
    # Ligne principale des √©v√©nements par jour
    fig.add_trace(go.Bar(
        x=data['date'],
        y=data['events_count'],
        name='Nombre d\'√©v√©nements',
        marker_color='rgba(155, 89, 182, 0.6)',
        hovertemplate='%{x|%d %b %Y}: %{y} √©v√©nements<extra></extra>'
    ))
    
    # Mise en forme du graphique
    fig.update_layout(
        title="Fr√©quence des √©v√©nements d'apprentissage",
        xaxis_title="Date",
        yaxis_title="Nombre d'√©v√©nements",
        hovermode="x unified",
        height=350,
        margin=dict(l=20, r=20, t=40, b=20)
    )
    
    return fig

def analyze_data_sources_impact():
    """
    Analyse l'impact des diff√©rentes sources de donn√©es sur la qualit√© des pr√©dictions.
    
    Returns:
        dict: Analyse de l'impact des sources de donn√©es
    """
    # Liste des sources de donn√©es
    data_sources = [
        "API Football",
        "Transfermarkt",
        "SoccerData (FBref)",
        "SoccerData (WhoScored)",
        "Donn√©es d√©taill√©es des joueurs",
        "Donn√©es des managers"
    ]
    
    # G√©n√©ration de m√©triques d'impact pour chaque source
    impact_metrics = {}
    
    for source in data_sources:
        # G√©n√©rer des m√©triques d'impact al√©atoires
        impact_metrics[source] = {
            "accuracy_improvement": round(random.uniform(0.02, 0.08), 3),
            "coverage": round(random.uniform(0.7, 0.98), 2),
            "reliability": round(random.uniform(0.75, 0.95), 2),
            "integration_level": round(random.uniform(0.6, 0.9), 2)
        }
    
    # Analyse comparative
    best_accuracy_source = max(impact_metrics.items(), key=lambda x: x[1]["accuracy_improvement"])[0]
    best_coverage_source = max(impact_metrics.items(), key=lambda x: x[1]["coverage"])[0]
    best_reliability_source = max(impact_metrics.items(), key=lambda x: x[1]["reliability"])[0]
    
    # Combinaisons synergiques
    synergies = [
        {
            "sources": ["API Football", "Transfermarkt"],
            "combined_improvement": round(random.uniform(0.08, 0.12), 3),
            "description": "Am√©lioration significative de la pr√©cision des pr√©dictions sur les transferts et leur impact sur les performances."
        },
        {
            "sources": ["SoccerData (FBref)", "Donn√©es d√©taill√©es des joueurs"],
            "combined_improvement": round(random.uniform(0.07, 0.11), 3),
            "description": "Analyse plus fine des performances individuelles et de leur contribution aux r√©sultats d'√©quipe."
        },
        {
            "sources": ["Transfermarkt", "Donn√©es des managers"],
            "combined_improvement": round(random.uniform(0.06, 0.1), 3),
            "description": "Meilleure compr√©hension de l'impact des changements d'entra√Æneurs et de leur style de jeu."
        }
    ]
    
    return {
        "metrics": impact_metrics,
        "best_sources": {
            "accuracy": best_accuracy_source,
            "coverage": best_coverage_source,
            "reliability": best_reliability_source
        },
        "synergies": synergies
    }

def display_enhanced_learning_system_tab():
    """
    Affiche l'onglet Syst√®me d'Apprentissage am√©lior√© complet.
    """
    st.markdown("## üß† Syst√®me d'Apprentissage")
    st.markdown("Visualisation de l'√©volution de l'intelligence d'ArcanShadow et ses processus d'apprentissage avec int√©gration multi-sources.")
    
    # G√©n√©ration des donn√©es d'apprentissage
    if "learning_data" not in st.session_state:
        st.session_state.learning_data = generate_learning_data()
    
    # Acc√®s aux donn√©es
    learning_data = st.session_state.learning_data
    daily_data = learning_data['daily_data']
    events = learning_data['events']
    modules = learning_data['modules']
    
    # S√©parer l'interface en deux colonnes principales
    col1, col2 = st.columns([2, 1])
    
    with col1:
        # Graphiques d'√©volution
        st.markdown("### √âvolution de l'intelligence du syst√®me")
        
        # Onglets pour les diff√©rents graphiques
        tabs = st.tabs(["Pr√©cision", "Patterns", "√âv√©nements"])
        
        with tabs[0]:
            st.plotly_chart(create_accuracy_chart(daily_data), use_container_width=True)
        
        with tabs[1]:
            st.plotly_chart(create_patterns_chart(daily_data), use_container_width=True)
        
        with tabs[2]:
            st.plotly_chart(create_learning_events_chart(daily_data, events), use_container_width=True)
    
    with col2:
        # M√©triques cl√©s
        st.markdown("### M√©triques cl√©s")
        
        # Derni√®re pr√©cision
        latest_accuracy = daily_data['accuracy'].iloc[-1]
        accuracy_diff = daily_data['accuracy'].iloc[-1] - daily_data['accuracy'].iloc[-7]
        
        accuracy_color = "green" if accuracy_diff >= 0 else "red"
        st.markdown(f"""
        <div style="background-color: rgba(94, 75, 139, 0.1); padding: 10px; border-radius: 5px; margin-bottom: 15px;">
            <h4>Pr√©cision actuelle</h4>
            <p style="font-size: 1.8em; font-weight: bold;">{latest_accuracy:.1%}</p>
            <p>
                <span style="color: {accuracy_color}; font-weight: bold;">
                    {'‚ñ≤' if accuracy_diff >= 0 else '‚ñº'} {abs(accuracy_diff):.1%}
                </span>
                <span style="color: gray;"> / 7 jours</span>
            </p>
        </div>
        """, unsafe_allow_html=True)
        
        # Nombre de patterns
        latest_patterns = daily_data['patterns'].iloc[-1]
        patterns_diff = daily_data['patterns'].iloc[-1] - daily_data['patterns'].iloc[-7]
        
        patterns_color = "green" if patterns_diff >= 0 else "red"
        st.markdown(f"""
        <div style="background-color: rgba(94, 75, 139, 0.1); padding: 10px; border-radius: 5px; margin-bottom: 15px;">
            <h4>Patterns identifi√©s</h4>
            <p style="font-size: 1.8em; font-weight: bold;">{latest_patterns}</p>
            <p>
                <span style="color: {patterns_color}; font-weight: bold;">
                    {'‚ñ≤' if patterns_diff >= 0 else '‚ñº'} {abs(patterns_diff)}
                </span>
                <span style="color: gray;"> / 7 jours</span>
            </p>
        </div>
        """, unsafe_allow_html=True)
        
        # Dernier √©v√©nement d'apprentissage
        if events:
            latest_event = events[-1]
            days_ago = (datetime.now() - latest_event['date']).days
            time_ago = f"il y a {days_ago} jour{'s' if days_ago > 1 else ''}"
            
            st.markdown(f"""
            <div style="background-color: rgba(94, 75, 139, 0.1); padding: 10px; border-radius: 5px; margin-bottom: 15px;">
                <h4>Dernier √©v√©nement d'apprentissage</h4>
                <p style="font-size: 1.2em; font-weight: bold;">{latest_event['type']}</p>
                <p>{latest_event['description']}</p>
                <p style="color: gray; font-size: 0.9em;">{time_ago}</p>
            </div>
            """, unsafe_allow_html=True)
    
    # Section sur les modules et leur performance
    st.markdown("### Performance des modules")
    
    # Graphique radar des performances
    st.plotly_chart(create_module_radar_chart(modules), use_container_width=True)
    
    # Tableau des √©v√©nements d'apprentissage r√©cents
    st.markdown("### √âv√©nements d'apprentissage r√©cents")
    
    if events:
        # Tri des √©v√©nements par date, du plus r√©cent au plus ancien
        sorted_events = sorted(events, key=lambda x: x['date'], reverse=True)[:10]
        
        for event in sorted_events:
            days_ago = (datetime.now() - event['date']).days
            time_ago = f"il y a {days_ago} jour{'s' if days_ago > 1 else ''}"
            
            # Couleur selon le type d'√©v√©nement
            if "Int√©gration" in event['type']:
                color = "rgba(26, 188, 156, 0.8)"
            elif "Recalibration" in event['type']:
                color = "rgba(241, 196, 15, 0.8)"
            elif "pattern" in event['type'].lower():
                color = "rgba(46, 134, 193, 0.8)"
            elif "Analyse" in event['type']:
                color = "rgba(155, 89, 182, 0.8)"
            else:
                color = "rgba(93, 173, 226, 0.8)"
            
            st.markdown(f"""
            <div style="border-left: 4px solid {color}; padding-left: 10px; margin-bottom: 15px;">
                <div style="display: flex; justify-content: space-between;">
                    <span style="font-weight: bold;">{event['type']}</span>
                    <span style="color: gray;">{time_ago}</span>
                </div>
                <p>{event['description']}</p>
            </div>
            """, unsafe_allow_html=True)
    else:
        st.info("Aucun √©v√©nement d'apprentissage enregistr√© pour le moment.")
    
    # Analyse de l'impact des sources de donn√©es
    st.markdown("### Impact des sources de donn√©es")
    
    # G√©n√©rer l'analyse des sources de donn√©es
    data_sources_impact = analyze_data_sources_impact()
    
    # Afficher un tableau de l'impact des sources
    sources_df = pd.DataFrame.from_dict({
        source: {
            "Am√©lioration pr√©cision": f"{metrics['accuracy_improvement']:.1%}",
            "Couverture": f"{metrics['coverage']:.0%}",
            "Fiabilit√©": f"{metrics['reliability']:.0%}",
            "Niveau d'int√©gration": f"{metrics['integration_level']:.0%}"
        }
        for source, metrics in data_sources_impact["metrics"].items()
    }).T
    
    st.dataframe(sources_df, use_container_width=True)
    
    # Afficher les meilleures sources
    st.markdown("#### Sources les plus performantes")
    cols = st.columns(3)
    
    with cols[0]:
        st.metric("Meilleure pr√©cision", data_sources_impact["best_sources"]["accuracy"])
    
    with cols[1]:
        st.metric("Meilleure couverture", data_sources_impact["best_sources"]["coverage"])
    
    with cols[2]:
        st.metric("Plus fiable", data_sources_impact["best_sources"]["reliability"])
    
    # Afficher les synergies
    st.markdown("#### Synergies entre sources de donn√©es")
    
    for synergy in data_sources_impact["synergies"]:
        source1, source2 = synergy["sources"]
        st.markdown(f"""
        <div style="background-color: rgba(0,0,0,0.02); padding: 10px; border-radius: 5px; margin-bottom: 10px;">
            <div style="display: flex; justify-content: space-between;">
                <span><b>{source1}</b> + <b>{source2}</b></span>
                <span style="color: green; font-weight: bold;">+{synergy['combined_improvement']:.1%}</span>
            </div>
            <p>{synergy['description']}</p>
        </div>
        """, unsafe_allow_html=True)

def add_enhanced_learning_system_tab(tab):
    """
    Ajoute l'onglet Syst√®me d'Apprentissage am√©lior√© √† l'application principale.
    
    Args:
        tab: Objet tab Streamlit
    """
    with tab:
        display_enhanced_learning_system_tab()